# **Projektbeschreibung: Omnidirektionaler ROS2-Roboter (V0.9)**

## 1\. Motivation & Projektidee

Alles begann mit der Leidenschaft f√ºr Robotik aus dem Mechatronik-Studium und dem enormen Spa√ü am Coden, den ich bei der Entwicklung von Fullstack-Webanwendungen f√ºr meinen Arbeitgeber entdeckt habe. Nach zwei erfolgreichen Web-Projekten wollte ich endlich etwas umsetzen, das beide Welten ‚Äì komplexe Software und anspruchsvolle Hardware ‚Äì verbindet. Da fertige Roboter-Kits nicht nur teuer sind, sondern ich auch langj√§hrige Erfahrung im 3D-Druck habe, war die Entscheidung f√ºr ein komplett selbst entwickeltes ("self-sourced") Projekt schnell gefallen.

Die Idee war also geboren: Ein Roboter, der mehr kann als nur vor und zur√ºck fahren. Er sollte autonom navigieren, Objekte und Personen erkennen und **sich pr√§zise fernsteuern lassen.** Diese Idee der Fernsteuerung wurde stark durch meine Erfahrungen aus einem **5G-Forschungsprojekt** gepr√§gt, wo ich die Testleitung f√ºr automatisiertes und teleoperiertes Fahren begleiten durfte. Der Austausch mit den Entwicklern eines stra√üenzugelassenen automatischen Kraftfahrzeugs und die Inspiration durch die Steuerzentralen f√ºr Flottenfahrzeuge und Luftfahrzeuge haben mir gezeigt, was in diesem Bereich m√∂glich ist. Und um dem Ganzen noch eine pers√∂nliche und humorvolle Note zu geben, kam ich auf die Idee mit dem Nerf-Dart-Launcher ‚Äì inspiriert von diesem super lustigen Video: [https://www.youtube.com/watch?v=8RKy\_i2h\_j8\&t=789s](https://www.youtube.com/watch?v=8RKy_i2h_j8&t=789s). Es ging mir darum, tief in die Programmierung einzutauchen und zu lernen, wie man Hardware und Software zu einem intelligenten System kombiniert.

-----

## 2\. Erste Ideen ‚Äì Die unsortierte Wunschliste

Ganz am Anfang stand noch kein technischer Plan, sondern nur eine Sammlung von Vorstellungen, was der Roboter k√∂nnen sollte. Aber nat√ºrlich kannte ich all die fancy Buzzwords aus den Medien und Social Media ‚Äì **SLAM, Face Recognition, Object Recognition, Path Planning, Autonomous Driving, Computer Vision** ‚Äì und es war klar: Ich wollte all den coolen Shit haben\! Ich wusste bei vielen Dingen noch nicht im Detail, wie ich sie umsetzen w√ºrde, aber grundlegend wusste ich, was es ist, was es macht und wie es technisch grob funktioniert.

Die unsortierte Wunschliste sah also so aus:

* **Antrieb mit vier R√§dern:** Von Anfang an stand f√ºr mich fest, dass die Bewegung mit 4 angetriebenen R√§dern stattfinden muss. Das war nicht nur eine Abweichung von der Norm, sondern auch die Vorstellung, mehr Traktion, mehr Stabilit√§t und einfach eine solidere Basis f√ºr all die geplanten Features zu haben.
* **Intelligenz statt Dummheit:** Er sollte nicht einfach nur Befehle ausf√ºhren und gegen das n√§chste Tischbein fahren, sondern seine Umgebung selbstst√§ndig wahrnehmen und sich darin orientieren k√∂nnen.
* **Interaktion statt nur Herumfahren:** Der Roboter sollte nicht nur ein passives Fahrzeug sein, sondern auch aktiv etwas tun k√∂nnen ‚Äì eine sichtbare, interaktive Funktion haben.
* **Eine "pers√∂nliche" Note:** Haben wir nicht alle auf die gro√üe KI-Integration gewartet, bei der man tiefgr√ºndige Gespr√§che mit seinem Roboter f√ºhrt? Genau das wollte ich nicht. Die Idee war eher, ihm lustige Spr√ºche beizubringen, die er einem im passenden Moment zuruft, oder ihn auf einfache Sprachbefehle reagieren zu lassen. Mehr Pers√∂nlichkeit, weniger "Skynet". üòâ
* **Volle Kontrolle und √úbersicht:** Ich wollte ihn nicht nur wie ein ferngesteuertes Auto fahren, sondern auch jederzeit sehen, was er "denkt" ‚Äì also Zugriff auf all seine Sensordaten und seinen Zustand haben, fast wie in einem Kontrollzentrum.
* **Ein h√§ufig genutztes Industrie-Fundament:** Das Ganze sollte nicht in einer Datei mit 10.000 Zeilen Code auf Arduino-Basis stehen, sondern auf einer Software-Architektur, die auch in der Industrie f√ºr Automatisierung und Robotik verwendet wird.

-----

## 3\. Konkretisierung der Vision

Aus dieser unsortierten Wunschliste wurden dann nach und nach die konkreten Kernfunktionen des Projekts geformt.

Meine erste Entscheidung, die aus der Idee einer Bewegung mit vier R√§dern entstand, war tats√§chlich eine aus Trotz (oder nennen wir es Experimentierfreude üòâ): **Es mussten vier R√§der und vier Motoren sein\!** Viele DIY-Projekte setzen auf den weit verbreiteten Differentialantrieb mit zwei Motoren. Ich wollte aber nicht die n√§chste Kopie des TurtleBots entwickeln.

Die vage Vorstellung von "Intelligenz", angetrieben durch die Buzzwords, wurde zur klaren Anforderung, dass der Roboter sich in meiner Wohnung zurechtfinden, selbstst√§ndig eine Karte erstellen und immer genau wissen muss, wo er ist. Damit war das Ziel **SLAM (Simultaneous Localization and Mapping)** gesetzt.

Aus dem Wunsch nach "Interaktion" und den Schlagw√∂rtern "Face Recognition" wurde die konkrete Idee mit dem **Nerf-Dart-Launcher**. Das Ziel war, ernste Technik wie **Computer Vision zur Gesichtserkennung** zu nehmen und sie mit einem interaktiven, greifbaren und lustigen Feature zu verbinden.

Die Anforderung der "vollen Kontrolle" wurde weiter pr√§zisiert: Die Steuerung findet **immer remote** statt, sei es manuell oder durch autonome Befehle. Au√üerdem war klar, dass die **Steuerung des Nerf-Launchers komplett getrennt von der Fahrsteuerung** erfolgen muss, um eine saubere und modulare Architektur zu gew√§hrleisten. Das m√ºndete in die Ziele einer direkten **Fernsteuerung per Xbox-Controller** und einem **Web-Dashboard** zur Visualisierung aller Telemetriedaten.

Und das "professionelle Fundament"? Da gab es keine Diskussion: Das Projekt musste von Anfang an auf **ROS2** aufbauen.

-----

## 4\. Recherche & Konzeptentscheidung

Nachdem klar war, *was* der Roboter k√∂nnen soll, ging die eigentliche Arbeit los: *Wie* zum Teufel setzt man das alles um? Dieser Prozess war eine Abfolge von Recherchen und Entscheidungen f√ºr jede Kernfunktion.

### 4.1. Recherche: Antriebskonzept

Die Entscheidung f√ºr vier R√§der √∂ffnete die T√ºr zu verschiedenen Antriebskonzepten. Aus meiner KFZ-Ausbildung war ich zwar mit "Steering"-Konzepten bestens vertraut, aber was gab das Roboter-Framework noch so her?

```mermaid
graph TD
    subgraph "Anforderung: Antrieb mit 4 R√§dern"
        A{Welche Bewegungsart wird angestrebt?}
        A -- "Maximale Man√∂vrierf√§higkeit (holonom)" --> B[Omnidirektionale Antriebe]
        A -- "Fahrzeug√§hnliche Bewegung (nicht-holonom)" --> C[Gelenkte / Starre Antriebe]
    end

    subgraph "Technologie-Optionen"
        B -- "F√ºr glatte B√∂den" --> B1["Mecanum-R√§der"]
        B -- "F√ºr unebene B√∂den" --> B2["Omni-R√§der"]
        C -- "Panzerlenkung" --> C1["Skid-Steer"]
        C -- "Autolenkung" --> C2["Ackerman-Lenkung"]
    end

    subgraph "Meine Entscheidung"
        B1 --> End1[Ergebnis: Mecanum-Antrieb f√ºr Indoor-Agilit√§t]
    end

    style End1 fill:#cde4ff,stroke:#333,stroke-width:2px
```

### 4.2. Recherche: Sensorik f√ºr SLAM

F√ºr die autonome Navigation musste der Roboter seine Umgebung "sehen" k√∂nnen. Die Frage war: womit?

```mermaid
mindmap
  root("(Sensor f√ºr SLAM?)")
    LiDAR
      ::icon(fa fa-satellite-dish)
      + Hohe Pr√§zision 2D
      + 360¬∞ Abdeckung
      + Unempfindlich gegen Licht
      - Kostenfaktor
    Tiefenkamera (Stereo/ToF)
      ::icon(fa fa-camera-retro)
      + G√ºnstiger
      + Echte 3D-Daten
      - Begrenztes Sichtfeld
      - Anf√§llig f√ºr Lichtverh√§ltnisse
    Ultraschall
      ::icon(fa fa-assistive-listening-systems)
      + Sehr g√ºnstig
      - Zu ungenau f√ºr SLAM
      - Nur simple Hinderniserkennung
    Entscheidung
      ::icon(fa fa-check-circle)
      **LiDAR** f√ºr robustes 2D-Mapping als Basis.
```

### 4.3. Recherche: Steuerungsarchitektur

Die Kombination aus rechenintensivem ROS2 und der Notwendigkeit einer pr√§zisen, ruckelfreien Motoransteuerung warf die Frage nach der Systemarchitektur auf.

```mermaid
flowchart LR
    subgraph "Option A: Alles auf einem Board"
        A1[Raspberry Pi 4]
        A1 --> A2{"ROS2 (High-Level)"}
        A1 --> A3{"Motor-PWM (Low-Level)"}
        style A3 fill:#f8d7da,stroke:#721c24
    end
    
    subgraph "Option B: Getrennte Architektur"
        B1[Raspberry Pi 4] --> B2{"ROS2 (High-Level)"}
        B1 -- "Befehle (z.B. fahre 0.5 m/s)" --> B3[Raspberry Pi Pico]
        B3 --> B4{"Motor-PWM (Echtzeit)"}
        style B4 fill:#d4edda,stroke:#155724
    end

    subgraph "Bewertung & Entscheidung"
        C1{"Problem bei A: Linux ist kein Echtzeit-OS. PWM-Signale k√∂nnen 'zittern' (Jitter), was zu unsauberer Bewegung f√ºhrt."}
        C2{"Vorteil von B: Der Pico √ºbernimmt die Echtzeit-kritischen Aufgaben und entlastet den Pi 4. Das sorgt f√ºr eine saubere und zuverl√§ssige Ansteuerung."}
        C3[**Entscheidung f√ºr Option B**]
    end

    A1 --> C1
    B1 --> C2
    C1 --> C3
    C2 --> C3
```

-----

## 5\. Systemarchitektur

Die Architektur des Roboters ist modular aufgebaut und nutzt die St√§rken verschiedener Controller f√ºr spezifische Aufgaben. Die Kommunikation zwischen den Komponenten erfolgt durchg√§ngig √ºber das ROS2-Netzwerk, wobei **micro-ROS** als Br√ºcke zu den Mikrocontrollern dient.

### 5.1. High-Level-Steuerung (Das Gehirn)

* **Komponente:** Raspberry Pi 4B (8GB)
* **Software:** Ubuntu Server, ROS2 Humble
* **Aufgaben:** Der Pi 4 ist der zentrale Knotenpunkt des ROS2-Systems und verantwortlich f√ºr alle rechenintensiven Aufgaben:
  * **Verarbeitung der Daten** von LiDAR und Kamera.
  * **Ausf√ºhrung der SLAM-Algorithmen** zur Kartenerstellung und Lokalisierung.
  * **Pfadplanung** f√ºr die autonome Navigation.
  * **Hosting des Web-Dashboards** und der ROS2-Web-Bridge.
  * **Ausf√ºhrung der Computer-Vision-Software** zur Gesichtserkennung.
* **Kommunikation:** Der Pi 4 betreibt den **`micro-ros-agent`**, der eine nahtlose Br√ºcke zwischen dem Haupt-ROS2-Netzwerk und den angebundenen Mikrocontrollern (micro-ROS Clients) herstellt. Alle Befehle und Sensordaten werden √ºber ROS2-Topics ausgetauscht.

### 5.2. Low-Level Echtzeit-Steuerung (Das R√ºckgrat)

* **Komponente:** Raspberry Pi Pico
* **Software:** FreeRTOS, micro-ROS Client
* **Aufgaben:** Der Pico ist als harter Echtzeit-Knoten f√ºr die gesamte Bewegungssteuerung und die dazugeh√∂rige Sensorik zust√§ndig:
  * **Ansteuerung der 4 Motoren** inklusive Auslesen der **Encoder**.
  * **Auslesen der IMU- und ToF-Sensordaten.**
  * **Ver√∂ffentlichung der Sensor- und Odometrie-Daten** auf ROS2-Topics.
  * **Abonnieren von Bewegungsbefehlen** (Typ `geometry_msgs/Twist`), die vom Pi 4 oder einem Remote-PC gesendet werden.

### 5.3. Aktor-Steuerung (Der Sch√ºtze)

* **Komponente:** Arduino Nano oder Pro Micro
* **Software:** micro-ROS Client
* **Aufgaben:** Dieser dedizierte Mikrocontroller ist ausschlie√ülich f√ºr die Steuerung des **Nerf-Launchers** verantwortlich. Dies entkoppelt die interaktive Funktion von der kritischen Bewegungssteuerung.
  * Ansteuerung der **Pan/Tilt-Servos**.
  * Steuerung der **Flywheel-Motoren** zum Beschleunigen der Darts.
  * Ausl√∂sen des **Abzugsmechanismus**.
  * Abonniert Ziel- und Feuerbefehle von einem eigenen ROS2-Topic.

### 5.4. Remote-Steuerung (Die Kommandozentrale)

* **Komponente:** Externer PC
* **Aufgaben:** Sowohl die manuelle Steuerung als auch die √úberwachung finden nicht direkt am Roboter, sondern von einer Remote-Workstation aus statt.
  * Der **Xbox-Controller** ist mit dem PC verbunden. Eine ROS2-Node auf dem PC wandelt die Controller-Eingaben in `Twist`-Nachrichten um und sendet sie √ºber das WLAN an den Roboter.
  * Das **Web-Dashboard** wird im Browser auf dem PC aufgerufen und kommuniziert √ºber die ROS2-Web-Bridge mit dem Roboter, um Telemetriedaten anzuzeigen und Befehle zu senden.

-----

## 6\. Technische Komponenten

Die folgende Tabelle listet die zentralen Hardwarekomponenten auf, die auf Basis des Rechercheprozesses ausgew√§hlt wurden, um die definierte Systemarchitektur umzusetzen.

| **Kategorie**             | **Komponente**                                       | **Zweck**                                                             |
| :------------------------ | :--------------------------------------------------- | :-------------------------------------------------------------------- |
| **Chassis & Antrieb**     | 4x DC-Getriebemotoren (GM3865-520) mit Hall-Encodern | Kraftvoller Antrieb und Feedback zur Raddrehung                       |
|                           | 4x 80mm Mecanum-R√§der                                | Erm√∂glichen die omnidirektionale Bewegung                             |
|                           | 4x TB6612FNG Motortreiber                            | Ansteuerung der DC-Motoren                                            |
| **Steuerung & Sensorik**  | Raspberry Pi 4B (8GB)                                | High-Level-Steuerung, ROS2, Computer Vision                           |
|                           | Raspberry Pi Pico                                    | Low-Level-Steuerung (Motoren, Encoder, IMU, ToF)                      |
|                           | Arduino Nano / Pro Micro                             | Dedizierte Steuerung des Nerf-Launchers                               |
|                           | Lidar LDS01RR                                        | 360¬∞-Umgebungsscans f√ºr SLAM                                          |
|                           | ICM-20948 (9-DoF IMU)                                | Erfassung von Beschleunigung, Rotation und Ausrichtung (Sensorfusion) |
|                           | VL53L0X Time-of-Flight-Sensor                        | Pr√§zise Abstandsmessung f√ºr Hinderniserkennung                        |
|                           | USB-Kamera (1080p)                                   | Video-Streaming und Input f√ºr die Gesichtserkennung                   |
| **Nerf-Launcher**         | 2x RS2205 Brushless-Motoren & 2x 40A ESCs            | Beschleunigung der Nerf-Darts                                         |
|                           | 1x Digital-Servo (22kg) & 1x 9g Servo                | Zielen des Launchers (Pan/Tilt)                                       |
| **Energieversorgung**     | 3S Li-Ion Akku-Pack (18650 Zellen)                   | Mobile Stromversorgung                                                |
|                           | 3S Batterieschutzplatine (BMS)                       | Schutz vor √úberladung, Tiefentladung und Kurzschluss                  |
|                           | INA3221 Sensor                                       | √úberwachung von Spannung und Stromverbrauch                           |
| **Bedienung & Interface** | Xbox Controller (am Remote-PC)                       | Manuelle Fernsteuerung                                                |
|                           | Web-Dashboard (am Remote-PC)                         | Telemetrie und Steuerung                                              |




[![](https://img.plantuml.biz/plantuml/dsvg/RP9DRi9038NtSmgBLQ8gBAgkmg842RMgG20YbKqsWPZ4Ol2W6GVQ7g07w1bwazwa9-aa_5RjSlIU_VpPqtUIMwatMWc9HaW5QxGrbowtoue4rr9R26QJz1PshWlJ-HY6oqhUa2HKggrMj8AWarpy-xpIzvqGf4BsocGhP1YIEED4QtIGroj0Ojp0mEIsw8GA89xbRhj0QrzXId2NSHiPF59PHIFSeGuple6n4MEZPeXWPFP7eUVBsGY2zMRn3u2fDGkDcnX6nFKBa1DO-yJpwD4axHwCyt7qH29euVlz0sfsUUsdG1ZW7ak_XyQ6NBG12hN32z300psWM6JsuLVy1w06estwXwINyPIMfXMlYCvx_GVesLX1ql6PO6r9QDskikbrf6aYaKNdJb2wa1vX4dQdk_xcuO9MIiL35hUia0fly50dFYXn__LPQUdvGvkjwdk__oe_)](https://editor.plantuml.com/uml/RP9DRi9038NtSmgBLQ8gBAgkmg842RMgG20YbKqsWPZ4Ol2W6GVQ7g07w1bwazwa9-aa_5RjSlIU_VpPqtUIMwatMWc9HaW5QxGrbowtoue4rr9R26QJz1PshWlJ-HY6oqhUa2HKggrMj8AWarpy-xpIzvqGf4BsocGhP1YIEED4QtIGroj0Ojp0mEIsw8GA89xbRhj0QrzXId2NSHiPF59PHIFSeGuple6n4MEZPeXWPFP7eUVBsGY2zMRn3u2fDGkDcnX6nFKBa1DO-yJpwD4axHwCyt7qH29euVlz0sfsUUsdG1ZW7ak_XyQ6NBG12hN32z300psWM6JsuLVy1w06estwXwINyPIMfXMlYCvx_GVesLX1ql6PO6r9QDskikbrf6aYaKNdJb2wa1vX4dQdk_xcuO9MIiL35hUia0fly50dFYXn__LPQUdvGvkjwdk__oe_)


[![](https://img.plantuml.biz/plantuml/dsvg/RPBFQW8n4CRlUOh1NboAj2Zqe8UghAtGjPIkNaelqPtD3iQ99DcBp-C3z04yrHThzWyenLlc-y9ylqncxBoqljnfuSbhX1JP6KjRoCyd2saoMGXHNn6KGxVJs1VpkHE1Bv23bL0y-Un40c1O7qTmbv0g_5IN64Gs7i5MGYW0xc5k2eGFUpuUo9497Vfnr5g3fyVV7vYiCzniIrQjkcBSxNFYJDEc08Kgn2RXs3JimPjn7ZaKo5aT9r0xhB6NX3doLkxGoWRnMe5kwj6YULYQbsXqY_NrV6BdIQ5jXLc8HHZ4lLWscHN059L5FdXqa5PSEUgijMfEoVdvg3Mf_uo1SAMzbMvJIAqRm2lX9bAAXGNLShKfNoM4SI7Bykhx6kkzj30FiqFBvD1kaRcPx0M4Cyg56CxOXrYaBFZ3Rwx_4gHsC9MENDFOcdNfgdVsuzv2xlV4QXJMnEp5EBIfjkaF)](https://editor.plantuml.com/uml/RPBFQW8n4CRlUOh1NboAj2Zqe8UghAtGjPIkNaelqPtD3iQ99DcBp-C3z04yrHThzWyenLlc-y9ylqncxBoqljnfuSbhX1JP6KjRoCyd2saoMGXHNn6KGxVJs1VpkHE1Bv23bL0y-Un40c1O7qTmbv0g_5IN64Gs7i5MGYW0xc5k2eGFUpuUo9497Vfnr5g3fyVV7vYiCzniIrQjkcBSxNFYJDEc08Kgn2RXs3JimPjn7ZaKo5aT9r0xhB6NX3doLkxGoWRnMe5kwj6YULYQbsXqY_NrV6BdIQ5jXLc8HHZ4lLWscHN059L5FdXqa5PSEUgijMfEoVdvg3Mf_uo1SAMzbMvJIAqRm2lX9bAAXGNLShKfNoM4SI7Bykhx6kkzj30FiqFBvD1kaRcPx0M4Cyg56CxOXrYaBFZ3Rwx_4gHsC9MENDFOcdNfgdVsuzv2xlV4QXJMnEp5EBIfjkaF)


[![](https://img.plantuml.biz/plantuml/dsvg/LP6nJiCm48PtFuNLgHb25wPse4L228aAYOgjYualpidEkN8kWFeyVGOcjhmObpfLORt_z_z_lbjtR1BtRL2C741lY1F2U-SZ7U70UdwhNuUJXLGeTROghFLyTW-Vu7fodKpkPeNc1aZ6JGoHOhkdUMk-RlJMqF3z8Ncf7auDEW_8nQnGMYzVMaAB2JnZ97CjXfMevuetOaWkzMJ_5BXWc1TFP6DCvo29sa9bg6_Bp3c-Xz21c7kIFBhOniQ_7h9Ogyb9M3LSNkbhqwGZz4weAOWsGJz20F8CUg77NQJ_r7LCFbVjFDfonv2RnhaspY1bTLDvIAqXAfybJPOuuUI0pAMM3yHTrG1QCIptTFdaLAZhuIVfl8Toup0W3oTUoOdYGQqbOxpeBm00)](https://editor.plantuml.com/uml/LP6nJiCm48PtFuNLgHb25wPse4L228aAYOgjYualpidEkN8kWFeyVGOcjhmObpfLORt_z_z_lbjtR1BtRL2C741lY1F2U-SZ7U70UdwhNuUJXLGeTROghFLyTW-Vu7fodKpkPeNc1aZ6JGoHOhkdUMk-RlJMqF3z8Ncf7auDEW_8nQnGMYzVMaAB2JnZ97CjXfMevuetOaWkzMJ_5BXWc1TFP6DCvo29sa9bg6_Bp3c-Xz21c7kIFBhOniQ_7h9Ogyb9M3LSNkbhqwGZz4weAOWsGJz20F8CUg77NQJ_r7LCFbVjFDfonv2RnhaspY1bTLDvIAqXAfybJPOuuUI0pAMM3yHTrG1QCIptTFdaLAZhuIVfl8Toup0W3oTUoOdYGQqbOxpeBm00)


<!-- @startuml
title Antriebskonzept (4 R√§der)

start
if ("Bewegungsart?") then ("holonom\n(max. Man√∂vrierf√§higkeit)")
  :Omnidirektionale Antriebe;
  if ("Untergrund?") then ("glatt/Indoor")
    :Mecanum-R√§der;
  else ("uneben")
    :Omni-R√§der;
  endif
else ("nicht-holonom\n(fahrzeug√§hnlich)")
  :Gelenkte / starre Antriebe;
  fork
    :Skid-Steer;
  fork again
    :Ackermann-Lenkung;
  end fork
endif

:Entscheidung:\nMecanum-Antrieb f√ºr Indoor-Agilit√§t;
stop
@enduml
 -->

<!-- @startuml
title Sensorik f√ºr SLAM

start
if ("Sensor f√ºr SLAM?") then ("LiDAR")
  :LiDAR; #LightBlue
  note right
    + Hohe Pr√§zision (2D)
    + 360¬∞ Abdeckung
    + Unempfindlich gegen Licht
    - Kosten
  end note
elseif ("Tiefenkamera")
  :Tiefenkamera (Stereo/ToF);
  note right
    + G√ºnstiger
    + Echte 3D-Daten
    - Begrenztes Sichtfeld
    - Lichtanf√§llig
  end note
else ("Ultraschall")
  :Ultraschall;
  note right
    + Sehr g√ºnstig
    - Zu ungenau f√ºr SLAM
    - Nur Hinderniserkennung
  end note
endif

:Entscheidung:\nLiDAR als Basis f√ºr robustes 2D-Mapping;
stop
@enduml
 -->


 <!-- @startuml
title Steuerungsarchitektur (ROS2 vs. PWM Echtzeit)

start
if ("Architekturwahl?") then ("Option A:\nAlles auf einem Board")
  :Raspberry Pi 4\nROS2 (High-Level)\n+ Motor-PWM (Low-Level);
  note right
    Problem: Linux ist kein Echtzeit-OS.
    PWM-Jitter ‚Üí unsaubere Bewegung.
  end note
else ("Option B:\nGetrennte Architektur")
  :Pi 4 ‚Üí ROS2 (High-Level);\nPico ‚Üí Motor-PWM (Echtzeit);
  note right
    Vorteil: Echtzeitkritik auf Pico.
    Pi 4 entlastet. Saubere Ansteuerung.
  end note
endif

:Entscheidung:\nOption B gew√§hlt;
stop
@enduml -->